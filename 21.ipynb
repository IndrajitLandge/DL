import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Set dataset directory (update this to your actual path)
DATA_DIR = '/path/to/tomato_leaf_dataset'  # <--- Replace this path

# Parameters
IMG_SIZE = (128, 128)
EPOCHS = 10
LEARNING_RATE = 0.0001

# Function to load and preprocess datasets
def load_dataset(batch_size):
    train_ds = tf.keras.preprocessing.image_dataset_from_directory(
        DATA_DIR,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=IMG_SIZE,
        batch_size=batch_size,
        label_mode='int'
    )
    val_ds = tf.keras.preprocessing.image_dataset_from_directory(
        DATA_DIR,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=IMG_SIZE,
        batch_size=batch_size,
        label_mode='int'
    )

    # Convert datasets to numpy for manual normalization
    train_images, train_labels = [], []
    for imgs, labels in train_ds:
        train_images.append(imgs)
        train_labels.append(labels)
    train_images = tf.concat(train_images, axis=0).numpy() / 255.0
    train_labels = tf.concat(train_labels, axis=0).numpy()

    val_images, val_labels = [], []
    for imgs, labels in val_ds:
        val_images.append(imgs)
        val_labels.append(labels)
    val_images = tf.concat(val_images, axis=0).numpy() / 255.0
    val_labels = tf.concat(val_labels, axis=0).numpy()

    return (train_images, train_labels), (val_images, val_labels), train_ds.class_names

# Define a CNN model
def build_model(num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(*IMG_SIZE, 3)),
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train with different batch sizes
histories = {}
for batch_size in [32, 64]:
    print(f"\n--- Training with Batch Size {batch_size} ---")
    (x_train, y_train), (x_val, y_val), class_names = load_dataset(batch_size)
    model = build_model(num_classes=len(class_names))
    history = model.fit(x_train, y_train, validation_data=(x_val, y_val),
                        epochs=EPOCHS, batch_size=batch_size)
    histories[batch_size] = history

# Plot accuracy comparison
plt.figure(figsize=(10, 6))
for batch_size, history in histories.items():
    plt.plot(history.history['val_accuracy'], label=f'Batch {batch_size}')
plt.title('Validation Accuracy: Batch Size 32 vs 64')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()
