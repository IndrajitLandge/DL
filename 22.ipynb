import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Set dataset directory (update this to your dataset path)
DATA_DIR = '/path/to/peach_leaf_dataset'  # <-- Replace with actual path

# Parameters
IMG_SIZE = (128, 128)
EPOCHS = 10
BATCH_SIZE = 32
LEARNING_RATE = 0.001

# Function to load and normalize dataset without .map()
def load_dataset():
    train_ds = tf.keras.preprocessing.image_dataset_from_directory(
        DATA_DIR,
        validation_split=0.2,
        subset="training",
        seed=42,
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        label_mode='int'
    )
    val_ds = tf.keras.preprocessing.image_dataset_from_directory(
        DATA_DIR,
        validation_split=0.2,
        subset="validation",
        seed=42,
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        label_mode='int'
    )

    # Convert to numpy arrays and normalize
    def ds_to_numpy(ds):
        images, labels = [], []
        for x, y in ds:
            images.append(x)
            labels.append(y)
        x = tf.concat(images, axis=0).numpy() / 255.0
        y = tf.concat(labels, axis=0).numpy()
        return x, y

    x_train, y_train = ds_to_numpy(train_ds)
    x_val, y_val = ds_to_numpy(val_ds)
    class_names = train_ds.class_names

    return (x_train, y_train), (x_val, y_val), class_names

# Define CNN model
def build_model(num_classes, optimizer):
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(*IMG_SIZE, 3)),
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Load the dataset
(x_train, y_train), (x_val, y_val), class_names = load_dataset()
num_classes = len(class_names)

# Define optimizers
optimizers = {
    "Adam": tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    "RMSprop": tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE)
}

histories = {}

# Train and evaluate models
for name, opt in optimizers.items():
    print(f"\nTraining with {name} optimizer...")
    model = build_model(num_classes, opt)
    history = model.fit(x_train, y_train,
                        validation_data=(x_val, y_val),
                        epochs=EPOCHS,
                        batch_size=BATCH_SIZE)
    histories[name] = history

# Plot validation accuracy and loss
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
for name, hist in histories.items():
    plt.plot(hist.history['val_accuracy'], label=f'{name}')
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss
plt.subplot(1, 2, 2)
for name, hist in histories.items():
    plt.plot(hist.history['val_loss'], label=f'{name}')
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
